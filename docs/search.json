[
  {
    "objectID": "index.html#meeting-objectives",
    "href": "index.html#meeting-objectives",
    "title": "Lab Meeting 20240626 - CLI and OSC",
    "section": "Meeting objectives",
    "text": "Meeting objectives\n- Learn how OSC is organized\n- Log into OSC on demand and learn how to navigate code server\n- Learn some basic UNIX commands\n- Learn some best file organization practices within the OSC\n- Create your own user directory within our OSC project\nI’m not sure how far we’ll get, but by the end of these lab meetings my goal is to have you introduced to the idea of using software in the command line at OSC\n\n\n\n\n\n\n(Re)Sources\n\n\n\n\n\nMuch of this is based directly on\n\nPLNTPTH 6193 - Practical Computing Skills for Omics Data taught by Jelmer Poelstra (which I highly reccomend)\nand a little from the COMS 2022 bioinformatics webinar series on HPC which can be found here"
  },
  {
    "objectID": "index.html#osc-structure",
    "href": "index.html#osc-structure",
    "title": "Lab Meeting 20240626 - CLI and OSC",
    "section": "OSC structure",
    "text": "OSC structure\n\n\n\nExample of a Node\n\n\n\nWords to know when taking about supercomputers\n\n\n\n\n\n\nTerms\n\n\n\n\n\n\nSupercomputer (aka “compute cluster” or just “cluster): a group of many computers connected through a high speed network\n\n(OSC has 2; “Owens” and “Pitzer”)\n\nNode: a single computer or server\nCore (aka processor/ CPU): a single unit that runs a set of instructions"
  },
  {
    "objectID": "index.html#osc-organization",
    "href": "index.html#osc-organization",
    "title": "Lab Meeting 20240626 - CLI and OSC",
    "section": "OSC Organization",
    "text": "OSC Organization\n\n\n\nOSC Organization Structure\n\n\n\n\n\n\n\n\nParts of OSC\n\n\n\n\n\n\nFile Systems: Where files are stored - this is shared between OSC systems\nLogin Nodes: a few “lower powered” computers that everyone shares after logging in\nCompute Nodes: The computers used to run analysis (e.g., slurm batch scripts)\n\n\n\n\n\nOSC Filing system\nMany directories where files are stored, all with different storage, speed, and back-ups\n\n\n\nOSC Filing System\n\n\n\n\n\n\n\n\nOSC Filing Systems\n\n\n\n\n\n\nHome (/users/) : These are your files only for you. Is backed up\nProject (/fs/ess/) : Shared location for project. Is backed up\nScratch (/fs/scratch/): Stores large input/ output files, is faster than Home or Project. Is NOT backed up."
  },
  {
    "objectID": "index.html#bash-commands",
    "href": "index.html#bash-commands",
    "title": "Lab Meeting 20240626 - CLI and OSC",
    "section": "BASH Commands",
    "text": "BASH Commands\nInstead of clicking on things to run them such as in a graphical user interface (GUI), command line requires, well, commands to do anything\n\n\n\nanatomy of a command\n\n\nUNIX comes with lots of commands (similar to functions in R or Python), but lets start out slow\n\nBaby’s first command: pwd\nLets start with an easy and useful command - pwd which prints the working directory\nType it into the terminal, press enter and see what happens\npwd\n/fs/ess/PAS1640\nEssentially what is happening is we give some input and the output is printed onto the screen which is the default for most UNIX commands\n\n\nCommand with arguments: cal\nHere is another easy example of a command in CLI - cal\ncal\n      June 2024     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\nType in, press enter, get a calendar printed out\nBut we can modify what this command does with arguments\n\n\n\n\n\n\nArguments\n\n\n\n\n\nArguments (also sometimes called flags) come after a command and are preceded by a dash (-) and most often are a single letter\n\n\n\nWith the -j argument using cal we can get printed the Julian calendar instead\ncal -j\n         June 2024         \nSun Mon Tue Wed Thu Fri Sat\n                        153\n154 155 156 157 158 159 160\n161 162 163 164 165 166 167\n168 169 170 171 172 173 174\n175 176 177 178 179 180 181\n182\n\n\nGetting help with -h\nMost commands have multiple arguments, some of which are required for the command to work. We can usually check what a command does and what its arguments are with the -h argument\nLet’s try it with cal\ncal -h\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\nFrom this we can see cal has a lot of different options. Just for funsies lets look at using the -3 which prints three monts AND the -j argument simultainiously\nhere, we put the arguments next to each other in any order\ncal -3 -j\n          May 2024                    June 2024                    July 2024         \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n            122 123 124 125                          153      183 184 185 186 187 188\n126 127 128 129 130 131 132  154 155 156 157 158 159 160  189 190 191 192 193 194 195\n133 134 135 136 137 138 139  161 162 163 164 165 166 167  196 197 198 199 200 201 202\n140 141 142 143 144 145 146  168 169 170 171 172 173 174  203 204 205 206 207 208 209\n147 148 149 150 151 152      175 176 177 178 179 180 181  210 211 212 213            \n                             182  \nArguments can also be passed after the same dash\ncal -3j\n          May 2024                    June 2024                    July 2024         \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n            122 123 124 125                          153      183 184 185 186 187 188\n126 127 128 129 130 131 132  154 155 156 157 158 159 160  189 190 191 192 193 194 195\n133 134 135 136 137 138 139  161 162 163 164 165 166 167  196 197 198 199 200 201 202\n140 141 142 143 144 145 146  168 169 170 171 172 173 174  203 204 205 206 207 208 209\n147 148 149 150 151 152      175 176 177 178 179 180 181  210 211 212 213            \n                             182  \n\n\n\n\n\n\nSpell it out\n\n\n\n\n\nArguments can (but not always) be written out following two preceding dashes (–)\ne.g., -h can be specified out with --help\ndepending on the command, it can require the spelled out or the shortened form, but most basic commands should have both available"
  },
  {
    "objectID": "index.html#commands-i-really-need-you-to-know",
    "href": "index.html#commands-i-really-need-you-to-know",
    "title": "Lab Meeting 20240626 - CLI and OSC",
    "section": "Commands I really need you to know",
    "text": "Commands I really need you to know\n\nCommands that do stuff: cd\nSo far the commands we have used only print something to the screen\nMany commands perform some type of action though, such as cd which allows us to change our current working dir\nlets first see where we are again\npwd\n/fs/ess/PAS1640\ncd takes a dir as an argument and takes us from our current wd to whatever dir we specify\nlets use cd to get to the auspiciously named practice_location/\ncd practice_location/\n\n\n\n\n\n\nAutocomplete with tab\n\n\n\n\n\ninstead of spelling out the full input everytime, you can use tab to autocomplete a line\ntry typing cd p and then pressing tab to try\nIf there are multple dir/ files with the start, double tapping tab will give a list of files that begin with what you have typed so far so you can select which you want to go to\n\n\n\npwd\n/fs/ess/PAS1640/practice_location\nand now we can move around!\nHere we also used relative paths - because practice_location was only one level below, we only had to type out /practice_location and not the absolute dir /fs/ess/PAS1640/practice_location. It is good to get into the habbit of using relative paths instead of absolute ones in case we move anything around.\n\n\n\n\n\n\nMore on movement\n\n\n\n\n\nevery dir has a few hidden directories that are useful to get around\n\n. : is the dir of your current dir\n.. : is the dir directly above you\n\nto go back a level, use cd ..\n\n\n\nif we want to go further into the dir, we can spell out the whole dir to get to the level_3 dir\ncd level_1/level_2/level_3/\nlets check\npwd\nand to get back out, we can use ..\ncd ../../../../\n\n\nMaking directories with mkdir\nmkdir takes a dir as an argument and creates it if one does not already exist\n# don't run this, its just an example of how you would use it\nmkdir &lt;/new_dir&gt;\n\n\n\n\n\n\nNote\n\n\n\nYou can make multiple subdirectories by adding the -p argument to mkdir\n# don't run this, its just an example of how you would use it\nmkdir -p &lt;/new_dir/new_subdir/new_sub-subdir&gt;\n\n\nfor practice, lets navigate to the users folder and make our own folder!\n\n\n\n\n\n\nWarning\n\n\n\nGet to the \\users dir without me showing you!!\n\n\nSolution (click here)\n\nfrom within \\PAS1640\ncd \\users\n\n\n\n\nMaking your own dir\nWithin the \\users dir we can make our own directories!!\ncall it whatever you want, but I personally recommend using\nmkdir $USER\n\n\n\n\n\n\nA tiny bit on variables\n\n\n\n\n\nWhile I’m skipping a detailed explanation on variables, I’ll quickly mention them here\nin UNIX, variables are kept in objects beginning with a dollar sign ($)\nthere are a few default ones such as\n\n$HOME : stores the name of the home dir\n$USER : stores the name of the user\n\nto see what a variable contains, try the echo function\n# this is a general example, don't run it\necho$&lt;VARIABLE&gt;\necho$USER\n\n\n\n\n\n\ntouch - update or create\ntouch has a little bit of an odd name for what it is most often used for.\nthe primary intended use of touch is to “update the access and modification times of each FILE to the current time”, but in my experince is used more for its secondary function of creating an empty file if one does not already exist.\nuse\n# this is a general example, don't run it\ntouch &lt;file&gt;.&lt;extension&gt;\ne.g.,\ntouch README.md\nUse this command to create a README file in your projects as a descriptor of whats there, part of good documentation!!\n\n\nls - list, what’s in a dir\nls can be run on its own to show what is in a dir\nuse\n# don't run this, its just a template example\nls\nls also has a bunch of useful arguments to increase its usability which I reccomend checking out, but here are a few I like\n\n\n\n\n\n\nimportant arguments for ls\n\n\n\n\n\n-l : long list formatting (looks better IMO)\n-t : sorts lists by time, showing the newest first\n-c : with -t, orders by last modified\n-r : reverses order shown, useful when used alongside -lt\n-a, --all : shows all files, including hidden ones\nhere’s probably the version I use the most\nls -lt\n\n\n\n\n\ncp - copy paste\ncp takes two files as arguments, copying the first to the second\ne.g.,\n# don't run this, its just a template example\ncp &lt;/from/&gt; &lt;/to/&gt;\nusing cp on dirs requires the -r argument\n\n\n\n\n\n\nimportant arguments for cp\n\n\n\n\n\n-r, --recursive : required when copying all contents of a dir from one to another\n\n\n\n\n\nmv - move\nsimilar to cp, mv takes two files as arguments, but instead of just coppying, it coppies the file and then deletes the old one\nuse\n# don't run this, its just a template example\nmv &lt;/from_old_place/&gt; &lt;/to_new_place/&gt;\nit again uses the -r argument if you want to move a dir containing any number of files or dirs\n\n\n\n\n\n\nimportant arguments for mv\n\n\n\n\n\n-r, --recursive : required when moving all contents of a dir from one to another\n\n\n\n\n\nrm - remove\nas the name implies, using rm removes a file. This can be a dangerous function as files are not always backed up, so use with caution!\nTo remove a dir we again need to use the -r argument\n\n\n\n\n\n\nimportant arguments for `rm``\n\n\n\n\n\n-r, --recursive : required when removing all contents of a dir\n-i : requires a prompt to remove a file, makes it much safer\n-f, -force : forces a remove, be careful when using this\n\n\n\n\n\nchmod - change file permissions\nchmod allows you to change file permission access for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x).\nchmod takes a user (either file owner(u), group(g), or everyone(a)) and adds or subtracts permissions for a given file\nuse\n# don't run this, its just a template example\nchmod &lt;user&gt; &lt; + or - &gt; &lt;permission&gt; &lt;file&gt;\ne.g.,\n# don't run these, they are just examples of use cases\n\n# this will make a script executable for the file owner\nchmod u+x script.sh\n\n# this will make data read only\nchmod a=r data/raw/*\nlets take a look at what the file permissions look like for the PAS1640 home dir\ncd /fs/ess/PAS1640\n\nls -lt\ntotal 20\ndrwxr-xr-x  3 awiedemer673 PAS1640  4096 Jun 25 01:38 users\ndrwxr-xr-x  3 awiedemer673 PAS1640  4096 Jun 25 01:26 practice_location\ndrwxr-xr-x  3 msholola     PAS0471  4096 Apr 17 20:34 maria\ndrwxr-xr-x 12 msholola     PAS0471 16384 Mar 22 10:55 plasma_transcriptomics\ndrwxr-xr-x  4 cooperstone  PAS1542  4096 Mar 14 13:51 jess\ndrwxr-xr-x  6 quiroz1      PAS1640  4096 Jun  2  2023 instrument_files\ndrwxr-xr-x  7 quiroz1      PAS1640  4096 May 13  2023 Apples_DIA\ndrwxr-xr-x  3 quiroz1      PAS1640  4096 Jun  6  2022 Apples_neg\nthe string of seemingly random letters and dashes denotes file permissions (e.g., drwxr-xr-x)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#some-quick-best-practices",
    "href": "index.html#some-quick-best-practices",
    "title": "Lab Meeting 20240626 - CLI and OSC",
    "section": "Some quick best practices",
    "text": "Some quick best practices\n\nUse one dir per preject!\n\n\n\n\n\n\nSeparate different kinds of files using a consistent dir structure\nSeparate\n\nRaw data (treat as read-only, don’t wanna directly mess with it - use chmod a=r data/raw/*)\nScripts\nOutputs (results) (treat as disposable and possible to regenerate - make outputs reproducable)\n\n\n\n\nUse relative paths whenever able\n\n\n\n\n\nif you use mv the absolute path changes, but not necessarily relative paths\n\nagain, . is where you are, .. signifies the path one up in the hierarchy\n\n\n\nFile naming\n3 good principals for file naming\n- Machine readable\n- Human readable\n- Ordered in a consistent way\n\nMachine readable\nIn file names provide metadata such as sample ID, date, treatment separated by underscores (_)\ne.g.,\n- astro_30S_har2_rep1.csv\n- rariety_treatment_replicate\nls astro*\nastro_00S_har1_rep1.csv\nastro_10S_har1_rep1.csv\nastro_20S_har1_rep1.csv\nastro_30S_har1_rep1.csv\netc...\nls *har2*\nastro_00S_har2_rep1\nesme_10S_har2_rep2\netc...\n\n\nCombining machine and human readability\n- Use underscores (_) to separate distinct units (sample name, metadata)\n- If you must separate words, use dashes (-) (i.e., wild-type)\n- Limit period use (.) to file extensions (eg., .txt, .csv)\n- (generally) __Never use spaces__\n\n\nDefault ordering\n- Use leading zeros when ordering numbers (005, 099)\n- Dates should be YYYY-MM-DD\n- Group similar files together by using the same phrase and number scripts by extension order \ne.g.,\nAP-01_normalize.R\nAP-02_log-transform.R\nAP-03_test-sig.R\n\n\n\nDocument everying\nUse a README.md file to explain whats in what"
  },
  {
    "objectID": "page_2.html",
    "href": "page_2.html",
    "title": "Using Software in OSC",
    "section": "",
    "text": "Learn how programs at OSC work\n\nAvailable programs\nUnavailable programs\n\nConda enviroments\n\n\nGet introduced to how to transfer data\n\nSFTP\nGlobus\n\n\nAnalysis of omics data sets, especially in genomics and transcriptomics, often involves using a sequence of specialized software (e.g., within our group, use MSconvert -&gt; Mzmine -&gt; GNPS, R)\nMany analysis pipelines involve software accessible through the Unix shell (i.e., FastQC) and it is my goal to introduce you all to how utilize software in the Unix shell at OSC.\nObtaining and utilizing software at the OSC depends on weather or not the programs are already available through the OSC or are not and need imported. For the case of unavailable software, I’ll be showing how to use the Conda software management system.\nKnowing how to use software is great, but useless if we don’t know how to import our data to be analyzed, so I’ll also be going over a little of that today with SFTP and Globus ."
  },
  {
    "objectID": "page_2.html#meeting-objectives",
    "href": "page_2.html#meeting-objectives",
    "title": "Using Software in OSC",
    "section": "",
    "text": "Learn how programs at OSC work\n\nAvailable programs\nUnavailable programs\n\nConda enviroments\n\n\nGet introduced to how to transfer data\n\nSFTP\nGlobus\n\n\nAnalysis of omics data sets, especially in genomics and transcriptomics, often involves using a sequence of specialized software (e.g., within our group, use MSconvert -&gt; Mzmine -&gt; GNPS, R)\nMany analysis pipelines involve software accessible through the Unix shell (i.e., FastQC) and it is my goal to introduce you all to how utilize software in the Unix shell at OSC.\nObtaining and utilizing software at the OSC depends on weather or not the programs are already available through the OSC or are not and need imported. For the case of unavailable software, I’ll be showing how to use the Conda software management system.\nKnowing how to use software is great, but useless if we don’t know how to import our data to be analyzed, so I’ll also be going over a little of that today with SFTP and Globus ."
  },
  {
    "objectID": "page_2.html#checking-available-programs",
    "href": "page_2.html#checking-available-programs",
    "title": "Using Software in OSC",
    "section": "1.1 Checking available programs",
    "text": "1.1 Checking available programs\nThere are a few ways to do this!\n1. Just look it up [here](https://www.osc.edu/resources/available_software/software_list)\n2. Within the shell, we can see the available software with\n- __module spider__ : lists all installed modules\n- __module avail__ : lists all software that are currently available to be directly loaded given the current working environment\n\nExample - Lets look at the available versions of git\nfirst using module spider\n\nmodule spider git\n-------------------------------------------------------------------------------------------\n git:\n-------------------------------------------------------------------------------------------\n    Versions:\n       git/2.18.0\n       git/2.27.1\n       git/2.39.0\n\n-------------------------------------------------------------------------------------------\n For detailed information about a specific \"git\" module (including how to load the modules) use the module's full name.\n For example:\n\n    $ module spider git/2.39.0\n-------------------------------------------------------------------------------------------\n\nnext using module avail\n\nmodule avail git\n--------------------------------------- Global Aliases ----------------------------------------\n  intelmpi/2021.5.0 -&gt; intelmpi/2021.5    oneapi/2023.2.0 -&gt; intel-oneapi/2023.2.0\n\n------------------------------------ /apps/lmodfiles/Core -------------------------------------\n  git/2.18.0 (L,D)    git/2.27.1    git/2.39.0\n\n Where:\n  L:  Module is loaded\n  D:  Default Module\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the\n\"keys\".\n\nFrom this we can see a few key pieces of information\n1. Different versions of software are available and listed through OSC\n2. In this case, all of the installed modules match the available modules\n3. Only `module avail` gives information on what is the default program loaded (indicated with a `D`) and if it is already loaded (indicated with an `L`)\nThere also is actually a bunch of software already loaded by default in the OSC! At any point we can see what software is loaded by using module list\n\nmodule list\nCurrently Loaded Modules:\n1) xalt/latest               4) mvapich2/2.3.3     7) git/2.18.0\n2) gcc-compatibility/8.4.0   5) modules/sp2020     8) app_code_server/4.8.3\n3) intel/19.0.5              6) project/ondemand"
  },
  {
    "objectID": "page_2.html#module-spider-git2.39.0",
    "href": "page_2.html#module-spider-git2.39.0",
    "title": "Useing Software in OSC",
    "section": "> $ module spider git/2.39.0",
    "text": "&gt; $ module spider git/2.39.0\n\n```\n\nnext using module avail\n\nmodule avail git\n```bash-out ————————————— Global Aliases —————————————- intelmpi/2021.5.0 -&gt; intelmpi/2021.5 oneapi/2023.2.0 -&gt; intel-oneapi/2023.2.0\n———————————— /apps/lmodfiles/Core ————————————- git/2.18.0 (L,D) git/2.27.1 git/2.39.0\n\n\nWhere: L: Module is loaded D: Default Module\nUse “module spider” to find all possible modules. Use “module keyword key1 key2 …” to search for all possible modules matching any of the “keys”. ```\n\nFrom this we can see a few key pieces of information\n- Different versions of software are available and listed through OSC\n- In this case, all of the installed modules match the available modules\n- Only `module avail` gives information on what is the default program loaded (indicated with a `D`) and if it is already loaded (indicated with an `L`)\nThere also is actually a bunch of software already loaded by default in the OSC! At any point we can see what software is loaded by using module list &gt; bash &gt; module list &gt; &gt; bash-out &gt; Currently Loaded Modules:   1) xalt/latest               4) mvapich2/2.3.3     7) git/2.18.0   2) gcc-compatibility/8.4.0   5) modules/sp2020     8) app_code_server/4.8.3   3) intel/19.0.5              6) project/ondemand &gt;"
  },
  {
    "objectID": "page_2.html#loading-software",
    "href": "page_2.html#loading-software",
    "title": "Using Software in OSC",
    "section": "1.2 Loading software",
    "text": "1.2 Loading software\nWhat about software that is available, but is not already loaded?\nLets check out the module miniconda which is a useful program used to install outside programs that we’ll use later\nmodule avail miniconda\n--------------------------------------- Global Aliases ----------------------------------------\n   intelmpi/2021.5.0 -&gt; intelmpi/2021.5    oneapi/2023.2.0 -&gt; intel-oneapi/2023.2.0\n\n------------------------------------ /apps/lmodfiles/Core -------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py39     miniconda3/24.1.2-py310\n   miniconda3/4.12.0-py38        miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the\n\"keys\".\nWe can see that the default version is miniconda3/4.10.3-py37 (D), but it is not currently loaded\n\n\n\nloading modules with module load\nTo load available modules, we use module load\n# Load a module\nmodule load miniconda3 # loads the default version\nYou can also specify which version of a program you load\nmodule load miniconda3/4.10.3-py37 # loads a specific version - usually better to do for replicability\n\n\nunloading modules\nOccasionally, it may be useful to unload a module, such as when you have conflicting versions\n\nWe can unload a specific module with module unload &lt;module name&gt;\nTo unload all modules use module purge\n\n\n\n\n\n\n\nA note on software in scripts\n\n\n\n\n\nIf you’re ever running scripts, make sure the modules you want to use are loaded at the top of the script! (very similar to making sure your libraries are loaded in R)"
  },
  {
    "objectID": "page_2.html#conda-intro",
    "href": "page_2.html#conda-intro",
    "title": "Using Software in OSC",
    "section": "2.1 Conda Intro",
    "text": "2.1 Conda Intro\nWith conda, we can create what are called environments in which we can install one or more software packages\nAs long as a program is available in one of the online Conda repositories, which most bioinformatics programs are, it is fairly straightforward to download and run (similar to CRAN in R)\nA conda environment is essentially a dir that contains an executable program. I like to think of it essentially as an application.\nI generally recommend keeping your conda environments together\nHere are some I have\nls users/awiedemer673/conda/\nconda_program_list.md  mzmine  proteowizard kraken2_2.1.3\nEach environment is one separate program, similar to what we saw with the Lmod earlier (or think of it as my installed packages)"
  },
  {
    "objectID": "page_2.html#activating-a-conda-environment",
    "href": "page_2.html#activating-a-conda-environment",
    "title": "Using Software in OSC",
    "section": "2.2 Activating a Conda environment",
    "text": "2.2 Activating a Conda environment\nWhile the term load is used for the Lmod system, conda environments use the term activate - it really means the same thing, that we make a program ready to use\nSimilar to the module family, when using the conda environment we use the conda family of commands which includes\n\nconda activate : activates a conda environment\nconda deactivate : deactivates a conda environment\nconda create : creates a conda environment if one does not already exist\nconda install : intalls a conda environment\nconda update : updates a conda environment\n\ne.g.,\nconda activate ls users/awiedemer673/conda/proteowizard\nUnlike the Lmod system, the conda can only have one active environment at a time. If you activate another conda environment, the previous one will automatically be deactivated\nThis may seem like a hassle, but forces you to get in the habit of having a different script for every program you may use in your analysis pipeline\n\n\n\n\n\n\nconda only work if miniconda3 is running\n\n\n\n\n\nWe activate conda environments using the miniconda3 module we saw earlier, which is not pre-loaded into the OSC, so whenever we use a conda environment remember to module load miniconda3 !\n# if you are running a conda enviroment, it should look like this at some point\nmodule load miniconda3\n\nconda activate ./&lt;conda environment&gt;"
  },
  {
    "objectID": "page_2.html#creating-your-own-conda-enviroments",
    "href": "page_2.html#creating-your-own-conda-enviroments",
    "title": "Using Software in OSC",
    "section": "2.3 Creating your own conda enviroments",
    "text": "2.3 Creating your own conda enviroments\n\nOne-time setup\nBefore we can use conda to download anything, we need to do a little bit of setup - but don’t worry, this only needs to be done once.\nEssentially we need to set up what repositories we will have access to by using conda config\nconda config --add channels defaults   #added first -&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge #added last -&gt; highest priority\nLets check to see if we got the right repos\nconda config --get channels\nIt should return the following\n--add channels 'defaults' # lowest priority \n--add channels 'bioconda'\n --add channels 'conda-forge' # highest priority"
  },
  {
    "objectID": "page_2.html#conda-example---kraken2",
    "href": "page_2.html#conda-example---kraken2",
    "title": "Using Software in OSC",
    "section": "2.4 Conda Example - kraken2",
    "text": "2.4 Conda Example - kraken2\n\nLets practice downloading a conda envrironment using kraken2 as an example\n\nkraken2 is a piece of bioinformatics software that assigns taxonomy of metagenomic data using a k-mers based approach\n\n\n\nk-mers algorithm illustration\n\n\nI’m not going to get super into the specifics of how it works, but to get kraken2 to work you need to first set up a database to compare your data to, and then run the kraken2 software to see if there are any matches to the database.\nIf we look in the module list for kraken2, we can see its not there, requiring us to use the conda environment to use it\nmodule avail kraken2\n--------------------------------------- Global Aliases ----------------------------------------\n   intelmpi/2021.5.0 -&gt; intelmpi/2021.5    oneapi/2023.2.0 -&gt; intel-oneapi/2023.2.0\nNo modules found!\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the\n\"keys\".\n\n\n\nLets make a folder to store our conda enviroments\nBefore we download anything, lets first make a dir to store your conda environments\n\n\n\n\n\n\nA note on where to put your conda enviroments\n\n\n\n\n\nGenerally, there are two good options for where to keep your conda environments\n\nHave one large dir containing all of the conda environments you use in all of your projects (less hassle, but less reproducible if you want someone else to see what programs you use and how you got them)\nHave one conda environment containing dir per project with all of the relevant environments for that project (more work, but more reproducible)\n\nYou choose what you want to do - I have one big dir with all of my envrioments alongside a markdown document describing the code I used to download each\n\n\n\n\n\nSetup code\n# get to your user dir\ncd ./$USER/\n\n# make a folder called conda\nmkdir ./conda/\n\n\nInstalling kraken2\nNow lets install kraken2\ncd ./conda   #gets me to the place I want to install stuff in\nOf course load miniconda3\nmodule load miniconda3/24.1.2-py310\nlets get kraken\nconda create -y -p ./kraken2 -c bioconda kraken2\nBreaking down that code\n- `conda` is the family of commands we are using\n- `create` is the sub-command that is used to create an environment\n- The `-n` argument specifies the name you are going to give this environment. Here I have said `proteowizard` but should also likely have included what version it is in the name\n- `-c` says what repository to install the environment from, in this case bioconda\n- The end doesn't have an argument, but it’s the name of the program we want to install\n\n\n\n\n\n\n\nSoftware versions\n\n\n\n\n\nBy default, conda will install the latest available version of the program\nTo specify a version, use the equal sign after the program name\nconda create -y -p ./kraken2_2.1.3 -c bioconda kraken2=2.1.3\nThis is probably the better way to do it as you may run into trouble if you download a newer version of a program to the same name\n\n\n\n\n\nLet’s try it out!!\nconda activate ./kraken2_2.1.3\nAlso - notice how the command line has changed a little - you should notice you have something like this on your screen specifying that you have an activated conda environment\n/fs/ess/PAS1640/users/awiedemer673/conda/kraken2_2.1.3)\n\nNow we can use the kraken2 commands like any other - lets learn a little about it\nkraken2 --help\nUsage: kraken2 [options] &lt;filename(s)&gt;\n\nOptions:\n  --db NAME               Name for Kraken 2 DB\n                          (default: none)\n  --threads NUM           Number of threads (default: 1)\n  --quick                 Quick operation (use first hit or hits)\n  --unclassified-out FILENAME\n                          Print unclassified sequences to filename\n  --classified-out FILENAME\n                          Print classified sequences to filename\n  --output FILENAME       Print output to filename (default: stdout); \"-\" will\n                          suppress normal output\n  --confidence FLOAT      Confidence score threshold (default: 0.0); must be\n                          in [0, 1].\n  --minimum-base-quality NUM\n                          Minimum base quality used in classification (def: 0,\n                          only effective with FASTQ input).\n  --report FILENAME       Print a report with aggregrate counts/clade to file\n  --use-mpa-style         With --report, format report output like Kraken 1's\n                          kraken-mpa-report\n  --report-zero-counts    With --report, report counts for ALL taxa, even if\n                          counts are zero\n  --report-minimizer-data With --report, report minimizer and distinct minimizer\n                          count information in addition to normal Kraken report\n  --memory-mapping        Avoids loading database into RAM\n  --paired                The filenames provided have paired-end reads\n  --use-names             Print scientific names instead of just taxids\n  --gzip-compressed       Input files are compressed with gzip\n  --bzip2-compressed      Input files are compressed with bzip2\n  --minimum-hit-groups NUM\n                          Minimum number of hit groups (overlapping k-mers\n                          sharing the same minimizer) needed to make a call\n                          (default: 2)\n  --help                  Print this message\n  --version               Print version information\n\nIf none of the *-compressed flags are specified, and the filename provided\nis a regular file, automatic format detection is attempted.\n\nSo kraken2 has a lot of options, but we can see that its activated because it allows us to run the --help argument\n\n\n\nExiting a conda enviroment with conda deactivate\nTo deactivate a conda enviroment, all we need to use is conda deactivate - that’s literally it\n\n\n\n\n\n\nConda resources\n\n\n\n\n\nif you every want to check if a program is available through bioconda, check the website\n\nhttps://bioconda.github.io/index.html : the bioconda website, search for available programs\n\nIt also has programs such as proteowizard and MZmine"
  },
  {
    "objectID": "page_2.html#sftp",
    "href": "page_2.html#sftp",
    "title": "Using Software in OSC",
    "section": "3.1 SFTP",
    "text": "3.1 SFTP\nSFTP stands for secure file transfer protocol and is a general way to transfer files (if you’ve ever transferred data from MZmine to GNPS, you have used FTP which is similar). There are many GUI-based SFTP applications you can use, here I’ll show how to get FIleZilla as it works on most operating systems (and is the one Jelmer showed me how to use).\n\nSteps to get and use FIleZilla\n\nGo to the FileZilla download page and download the program toy your local system\nTo connect to OSC find the sit manager (file &gt; site manager)\nClick my sites\nIn General choose the following\n\nProtocol: SFTP - SSH File Transfer Protocol\nHost: sftp.osc.edu\nPort - leave blank\nLogon Type: Normal\nUser: your OSC user name\nPassword: Your OSC password\n\n\n\n\n\n\n\nTransferring files is now just drag and dropping it over to where you want\n\n\n\n\n\n\n\n\n\n\n\nMake sure you’re in the right place!!\n\n\n\n\n\nRemember how the filing system at OSC works! To get to the project go to /fs/ess/PAS1640 in the OSC side"
  },
  {
    "objectID": "page_2.html#globus",
    "href": "page_2.html#globus",
    "title": "Using Software in OSC",
    "section": "3.2 Globus",
    "text": "3.2 Globus\nI’m not going to detail how to use Globus here as Daniel Quiroz has made a great presentation on how to link globus specifically to the DPPC. His presentation can be found in Cooperstone Lab/Data processing PC/Globus/ Set up does require booking some time with the DPPC though.\nI will mention that the main advantage of Globus is that it is specifically useful for transferring large files and automatically updating them."
  }
]